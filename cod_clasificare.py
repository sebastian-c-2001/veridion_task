import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from tqdm import tqdm

# IniÈ›ializare model embeddings
model = SentenceTransformer("all-MiniLM-L6-v2")

# Citire date companii È™i taxonomie
company_df = pd.read_csv("date.csv")  # Se lucreazÄƒ direct pe `date.csv`
taxonomy_df = pd.read_csv("insurance_taxonomy.csv")

# Generare embeddings pentru taxonomie
taxonomy_df["embedding"] = taxonomy_df["label"].apply(lambda x: model.encode(x))
taxonomy_embeddings = np.vstack(taxonomy_df["embedding"])

# Setare ponderi pentru fiecare atribut
weights = {
    # "category": 0.2,  # Cel mai important
    # "niche": 0.2,
    # "description": 0.2,
    # "sector": 0.1,
    # "business_tags": 0.3  # Cel mai puÈ›in important

    "category": 0.2,  # Cel mai important
    "niche": 0.2,
    "description": 0.4,
    "sector": 0.2,
    "business_tags": 0  # Cel mai puÈ›in important
}

# FuncÈ›ie pentru generarea embedding-urilor ponderate
def compute_weighted_embedding(row):
    components = []
    total_weight = 0

    for col, weight in weights.items():
        if pd.notna(row[col]):
            embedding = model.encode(str(row[col]))
            components.append(weight * embedding)
            total_weight += weight

    return np.sum(components, axis=0) / total_weight if components else np.zeros(384)  # 384 = dimensiune embeddings

# AplicÄƒm pe toate companiile cu o barÄƒ de progres
company_embeddings = []
for _, row in tqdm(company_df.iterrows(), total=len(company_df), desc="Generating embeddings"):
    company_embeddings.append(compute_weighted_embedding(row))

company_embeddings = np.vstack(company_embeddings)

# Atribuire etichetÄƒ pe baza similaritÄƒÈ›ii cosinus
def assign_label(company_embedding):
    similarities = cosine_similarity([company_embedding], taxonomy_embeddings)
    best_match_idx = np.argmax(similarities)
    return taxonomy_df.iloc[best_match_idx]["label"], similarities[0, best_match_idx]

assigned_labels_and_scores = [assign_label(embedding) for embedding in tqdm(company_embeddings, desc="Assigning labels")]

# AdÄƒugÄƒm rezultatele Ã®n `date.csv`
company_df["assigned_label"] = [x[0] for x in assigned_labels_and_scores]

# SalvÄƒm `date.csv` cu noua coloanÄƒ
company_df.to_csv("date.csv", index=False)

# ğŸ”¹ DistribuÈ›ia claselor atribuite
taxonomy_distribution = company_df["assigned_label"].value_counts().reset_index()
taxonomy_distribution.columns = ["label", "count"]
taxonomy_distribution.to_csv("taxonomy_distribution.csv", index=False)

# ğŸ”¹ Calcul acurateÈ›e medie pe baza scorului de similaritate cosinus
mean_similarity_score = np.mean([x[1] for x in assigned_labels_and_scores])
print(f"âœ” AcurateÈ›ea medie bazatÄƒ pe similaritatea cosinus: {mean_similarity_score:.4f}")

# ğŸ”¹ Analiza distribuÈ›iei scorurilor de similaritate
similarity_scores = [x[1] for x in assigned_labels_and_scores]
similarity_stats = pd.Series(similarity_scores).describe()
print("âœ” DistribuÈ›ia scorurilor de similaritate:")
print(similarity_stats)

print("âœ” Toate fiÈ™ierele au fost actualizate cu succes!")
